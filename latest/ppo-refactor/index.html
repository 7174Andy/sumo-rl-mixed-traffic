
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Reinforcement learning for traffic flow optimization using SUMO">
      
      
      
      
        <link rel="prev" href="../algorithms/">
      
      
      
        
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.7.3">
    
    
      
        <title>PPO Refactoring - SUMO RL Mixed Traffic</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.484c7ddc.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.ab4e12ef.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#ppo-refactoring-from-cleanrl-style-to-stable-training" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="SUMO RL Mixed Traffic" class="md-header__button md-logo" aria-label="SUMO RL Mixed Traffic" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            SUMO RL Mixed Traffic
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              PPO Refactoring
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/7174Andy/sumo-rl-mixed-traffic" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="SUMO RL Mixed Traffic" class="md-nav__button md-logo" aria-label="SUMO RL Mixed Traffic" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    SUMO RL Mixed Traffic
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/7174Andy/sumo-rl-mixed-traffic" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Home
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../problem/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Problem Formulation
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../algorithms/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    
  
    Algorithms
  

    
  </span>
  
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    
  
    PPO Refactoring
  

    
  </span>
  
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    
  
    PPO Refactoring
  

    
  </span>
  
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#motivation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Motivation
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-cleanrl-style-refactoring" class="md-nav__link">
    <span class="md-ellipsis">
      
        The CleanRL-Style Refactoring
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The CleanRL-Style Refactoring">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#changes-made" class="md-nav__link">
    <span class="md-ellipsis">
      
        Changes Made
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrapper-chain" class="md-nav__link">
    <span class="md-ellipsis">
      
        Wrapper Chain
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results-unstable-training" class="md-nav__link">
    <span class="md-ellipsis">
      
        Results: Unstable Training
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Results: Unstable Training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#training-returns-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      
        Training Returns Comparison
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vehicle-speed-tracking" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vehicle Speed Tracking
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#root-cause-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      
        Root Cause Analysis
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Root Cause Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-cleanrl-conventions-didnt-transfer" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why CleanRL Conventions Didn't Transfer
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#final-implementation-hybrid-approach" class="md-nav__link">
    <span class="md-ellipsis">
      
        Final Implementation: Hybrid Approach
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Final Implementation: Hybrid Approach">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-was-kept-from-cleanrl" class="md-nav__link">
    <span class="md-ellipsis">
      
        What Was Kept from CleanRL
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-was-reverted" class="md-nav__link">
    <span class="md-ellipsis">
      
        What Was Reverted
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#action-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Action Scaling
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results-stable-training" class="md-nav__link">
    <span class="md-ellipsis">
      
        Results: Stable Training
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Results: Stable Training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#training-returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Training Returns
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        Training Metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vehicle-speed-tracking_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vehicle Speed Tracking
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Takeaways
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hyperparameter-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hyperparameter Tuning
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#motivation" class="md-nav__link">
    <span class="md-ellipsis">
      
        Motivation
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-cleanrl-style-refactoring" class="md-nav__link">
    <span class="md-ellipsis">
      
        The CleanRL-Style Refactoring
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The CleanRL-Style Refactoring">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#changes-made" class="md-nav__link">
    <span class="md-ellipsis">
      
        Changes Made
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#wrapper-chain" class="md-nav__link">
    <span class="md-ellipsis">
      
        Wrapper Chain
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results-unstable-training" class="md-nav__link">
    <span class="md-ellipsis">
      
        Results: Unstable Training
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Results: Unstable Training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#training-returns-comparison" class="md-nav__link">
    <span class="md-ellipsis">
      
        Training Returns Comparison
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vehicle-speed-tracking" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vehicle Speed Tracking
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#root-cause-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      
        Root Cause Analysis
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Root Cause Analysis">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#why-cleanrl-conventions-didnt-transfer" class="md-nav__link">
    <span class="md-ellipsis">
      
        Why CleanRL Conventions Didn't Transfer
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#final-implementation-hybrid-approach" class="md-nav__link">
    <span class="md-ellipsis">
      
        Final Implementation: Hybrid Approach
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Final Implementation: Hybrid Approach">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#what-was-kept-from-cleanrl" class="md-nav__link">
    <span class="md-ellipsis">
      
        What Was Kept from CleanRL
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#what-was-reverted" class="md-nav__link">
    <span class="md-ellipsis">
      
        What Was Reverted
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#action-scaling" class="md-nav__link">
    <span class="md-ellipsis">
      
        Action Scaling
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#results-stable-training" class="md-nav__link">
    <span class="md-ellipsis">
      
        Results: Stable Training
      
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Results: Stable Training">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#training-returns" class="md-nav__link">
    <span class="md-ellipsis">
      
        Training Returns
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#training-metrics" class="md-nav__link">
    <span class="md-ellipsis">
      
        Training Metrics
      
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#vehicle-speed-tracking_1" class="md-nav__link">
    <span class="md-ellipsis">
      
        Vehicle Speed Tracking
      
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#key-takeaways" class="md-nav__link">
    <span class="md-ellipsis">
      
        Key Takeaways
      
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#hyperparameter-tuning" class="md-nav__link">
    <span class="md-ellipsis">
      
        Hyperparameter Tuning
      
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              
              <article class="md-content__inner md-typeset">
                
                  


  
  


<h1 id="ppo-refactoring-from-cleanrl-style-to-stable-training">PPO Refactoring: From CleanRL-Style to Stable Training<a class="headerlink" href="#ppo-refactoring-from-cleanrl-style-to-stable-training" title="Permanent link">&para;</a></h1>
<p>This page documents the refactoring of the PPO implementation, the problems encountered, and the lessons learned about continuous action spaces in traffic control environments.</p>
<h2 id="motivation">Motivation<a class="headerlink" href="#motivation" title="Permanent link">&para;</a></h2>
<p>The original PPO implementation used <strong>tanh-squashed Gaussian policies</strong> with manual action scaling and a hardcoded reward divisor (<code>R /= 100</code>). While functional, the code had several issues:</p>
<ul>
<li>Tanh squashing required a manual Jacobian correction for log-probabilities</li>
<li>Action scaling from <span class="arithmatex">\([-1, 1]\)</span> to <span class="arithmatex">\([-3, 3]\)</span> was done outside the environment</li>
<li>The reward divisor was a magic constant with no adaptive behavior</li>
</ul>
<p>The goal was to modernize the PPO code following <a href="https://github.com/vwxyzjn/cleanrl">CleanRL</a> conventions, which have been shown to improve training stability and reproducibility across standard benchmarks (MuJoCo, Atari, etc.).</p>
<h2 id="the-cleanrl-style-refactoring">The CleanRL-Style Refactoring<a class="headerlink" href="#the-cleanrl-style-refactoring" title="Permanent link">&para;</a></h2>
<p>The refactoring applied the following changes:</p>
<h3 id="changes-made">Changes Made<a class="headerlink" href="#changes-made" title="Permanent link">&para;</a></h3>
<table>
<thead>
<tr>
<th>Component</th>
<th>Before</th>
<th>After (CleanRL)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>Action output</strong></td>
<td><code>tanh(sample)</code>, manual scale to <span class="arithmatex">\([-3, 3]\)</span></td>
<td>Raw Gaussian sample, <code>ClipAction</code> wrapper</td>
</tr>
<tr>
<td><strong>Log-probability</strong></td>
<td>Gaussian log-prob with Jacobian correction</td>
<td>Plain Gaussian log-prob</td>
</tr>
<tr>
<td><strong>Hidden activations</strong></td>
<td>ReLU</td>
<td>tanh</td>
</tr>
<tr>
<td><strong>Weight initialization</strong></td>
<td>PyTorch defaults</td>
<td>Orthogonal init (std=<span class="arithmatex">\(\sqrt{2}\)</span> for shared, 0.01 for actor, 1.0 for critic)</td>
</tr>
<tr>
<td><strong>Log-std clamping</strong></td>
<td>Clamped to <span class="arithmatex">\([-2.0, 0.5]\)</span></td>
<td>Unclamped</td>
</tr>
<tr>
<td><strong>Reward handling</strong></td>
<td><code>R /= 100</code> hardcoded</td>
<td><code>NormalizeReward</code> + <code>TransformReward(clip to [-10, 10])</code> wrappers</td>
</tr>
<tr>
<td><strong>Step API</strong></td>
<td>4-tuple <code>(obs, reward, done, info)</code></td>
<td>5-tuple via <code>FourToFiveTupleWrapper</code></td>
</tr>
</tbody>
</table>
<h3 id="wrapper-chain">Wrapper Chain<a class="headerlink" href="#wrapper-chain" title="Permanent link">&para;</a></h3>
<p>The refactored environment applies a chain of Gymnasium wrappers:</p>
<div class="highlight"><pre><span></span><code>RingRoadEnv -&gt; FourToFiveTupleWrapper -&gt; ClipAction -&gt; NormalizeReward -&gt; TransformReward
</code></pre></div>
<ul>
<li><strong>FourToFiveTupleWrapper</strong>: Bridges the 4-tuple <code>step()</code> API to Gymnasium's 5-tuple <code>(obs, reward, terminated, truncated, info)</code> without modifying the base environment.</li>
<li><strong>ClipAction</strong>: Clips actions to the environment's action space bounds <span class="arithmatex">\([-3, 3]\)</span>.</li>
<li><strong>NormalizeReward</strong>: Divides rewards by a running standard deviation for adaptive scaling.</li>
<li><strong>TransformReward</strong>: Clips normalized rewards to <span class="arithmatex">\([-10, 10]\)</span> to prevent outliers.</li>
</ul>
<h2 id="results-unstable-training">Results: Unstable Training<a class="headerlink" href="#results-unstable-training" title="Permanent link">&para;</a></h2>
<p>The refactored version showed significantly worse training stability compared to the original.</p>
<h3 id="training-returns-comparison">Training Returns Comparison<a class="headerlink" href="#training-returns-comparison" title="Permanent link">&para;</a></h3>
<p><strong>Original (tanh squashing):</strong></p>
<p><img alt="Original PPO Training Returns" src="../assets/images/ppo_training_returns.png" /></p>
<p><strong>CleanRL-style (raw Gaussian + ClipAction):</strong></p>
<p>The training curve showed frequent collapses throughout the entire run. Episodes that had been stable would suddenly drop to returns 5-10x worse, and the moving average never fully stabilized.</p>
<h3 id="vehicle-speed-tracking">Vehicle Speed Tracking<a class="headerlink" href="#vehicle-speed-tracking" title="Permanent link">&para;</a></h3>
<p><strong>Original:</strong></p>
<p><img alt="Original PPO Inference" src="../assets/images/ppo_inference.png" /></p>
<p><strong>CleanRL-style:</strong></p>
<p>The CAV exhibited large speed overshoots (5-6 m/s above the head vehicle) and oscillatory tracking behavior, indicating the learned policy was applying overly aggressive accelerations.</p>
<h2 id="root-cause-analysis">Root Cause Analysis<a class="headerlink" href="#root-cause-analysis" title="Permanent link">&para;</a></h2>
<p>The instability was traced to a <strong>feedback loop</strong> between the raw Gaussian policy and the <code>NormalizeReward</code> wrapper:</p>
<ol>
<li>
<p><strong>Raw Gaussian outputs extreme actions.</strong> Without tanh's natural bounding, the policy can produce acceleration values that swing abruptly between the full <span class="arithmatex">\(-3\)</span> and <span class="arithmatex">\(+3\)</span> m/s<span class="arithmatex">\(^2\)</span> range. The <code>ClipAction</code> wrapper hard-clips these, but provides <strong>zero gradient</strong> for out-of-bound outputs — the network cannot learn <em>how far</em> it overshot.</p>
</li>
<li>
<p><strong>Extreme actions cause collisions.</strong> Abrupt acceleration changes lead to unsafe following distances, triggering the <span class="arithmatex">\(-100\)</span> safety penalty in the reward function.</p>
</li>
<li>
<p><strong>Safety penalties destabilize <code>NormalizeReward</code>.</strong> The large penalty shifts the running reward statistics, distorting the reward scale for subsequent episodes.</p>
</li>
<li>
<p><strong>Distorted rewards cause bad policy updates.</strong> The value function estimates become inaccurate, leading to poor advantage estimates and further policy degradation.</p>
</li>
</ol>
<p>This cycle did not occur with tanh squashing because <strong>tanh naturally prevents extreme actions</strong>. The sigmoid-like shape means the network needs exponentially larger pre-activation values to push actions closer to the boundary, acting as an implicit regularizer.</p>
<h3 id="why-cleanrl-conventions-didnt-transfer">Why CleanRL Conventions Didn't Transfer<a class="headerlink" href="#why-cleanrl-conventions-didnt-transfer" title="Permanent link">&para;</a></h3>
<p>CleanRL's raw Gaussian approach works well for environments like MuJoCo (HalfCheetah, Hopper, etc.) because:</p>
<ul>
<li><strong>Smooth dynamics</strong>: Physics simulators have continuous, differentiable dynamics. An extreme action produces a proportionally bad outcome, not a discontinuous penalty.</li>
<li><strong>No hard safety penalties</strong>: MuJoCo rewards are typically smooth quadratic costs. There is no equivalent of the <span class="arithmatex">\(-100\)</span> collision penalty that can destabilize reward normalization.</li>
<li><strong>Dense reward signal</strong>: Every step provides informative gradient information. In the ring road environment, the reward signal is sparser and dominated by the safety constraint.</li>
</ul>
<p>The ring road environment has a <strong>discontinuous reward landscape</strong> (the <span class="arithmatex">\(-100\)</span> safety penalty creates a cliff) that interacts poorly with reward normalization when the policy can produce extreme actions.</p>
<h2 id="final-implementation-hybrid-approach">Final Implementation: Hybrid Approach<a class="headerlink" href="#final-implementation-hybrid-approach" title="Permanent link">&para;</a></h2>
<p>The final implementation keeps the beneficial CleanRL changes while restoring tanh squashing:</p>
<h3 id="what-was-kept-from-cleanrl">What Was Kept from CleanRL<a class="headerlink" href="#what-was-kept-from-cleanrl" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>Orthogonal weight initialization</strong> with appropriate scales (std=<span class="arithmatex">\(\sqrt{2}\)</span> for shared layers, 0.01 for actor, 1.0 for critic)</li>
<li><strong>tanh hidden activations</strong> instead of ReLU in shared layers</li>
<li><strong><code>NormalizeReward</code> + <code>TransformReward</code> wrappers</strong> for adaptive reward scaling</li>
<li><strong><code>FourToFiveTupleWrapper</code></strong> for Gymnasium 5-tuple API compatibility</li>
</ul>
<h3 id="what-was-reverted">What Was Reverted<a class="headerlink" href="#what-was-reverted" title="Permanent link">&para;</a></h3>
<ul>
<li><strong>tanh action squashing</strong> restored — actions are squashed to <span class="arithmatex">\([-1, 1]\)</span> then scaled by <code>max_accel</code></li>
<li><strong>Jacobian correction</strong> for log-probabilities restored:</li>
</ul>
<div class="arithmatex">\[
\log \pi(a \mid s) = \log \mu(u \mid s) - \sum_i \log(1 - \tanh^2(u_i))
\]</div>
<ul>
<li><strong>Log-std clamping</strong> to <span class="arithmatex">\([-2.0, 0.5]\)</span> restored to prevent exploration from becoming too large or too small</li>
</ul>
<h3 id="action-scaling">Action Scaling<a class="headerlink" href="#action-scaling" title="Permanent link">&para;</a></h3>
<p>The tanh output <span class="arithmatex">\(a_{\tanh} \in [-1, 1]\)</span> is scaled to the acceleration range using simple multiplication:</p>
<div class="highlight"><pre><span></span><code><span class="n">action_scaled</span> <span class="o">=</span> <span class="n">action_tanh</span> <span class="o">*</span> <span class="n">max_accel</span>  <span class="c1"># [-1, 1] * 3.0 -&gt; [-3, 3]</span>
</code></pre></div>
<p>This works because the action space is symmetric. The <code>ClipAction</code> wrapper remains in the chain as an additional safety net.</p>
<h2 id="results-stable-training">Results: Stable Training<a class="headerlink" href="#results-stable-training" title="Permanent link">&para;</a></h2>
<h3 id="training-returns">Training Returns<a class="headerlink" href="#training-returns" title="Permanent link">&para;</a></h3>
<p><img alt="Refactored PPO Training Returns" src="../assets/images/ppo_refactor_training_returns.png" /></p>
<p>The training curve converges by episode ~85 and remains stable, with only rare dips. The MA(10) shows consistent improvement without the periodic collapses seen in the CleanRL-style version.</p>
<h3 id="training-metrics">Training Metrics<a class="headerlink" href="#training-metrics" title="Permanent link">&para;</a></h3>
<p><img alt="Refactored PPO Training Metrics" src="../assets/images/ppo_refactor_training_metrics.png" /></p>
<ul>
<li><strong>Policy loss</strong>: Near zero with isolated spikes that recover quickly.</li>
<li><strong>Value loss</strong>: Drops from 0.25 to near 0, stable after update 100.</li>
<li><strong>Entropy</strong>: Healthy fluctuation around 1.5 — the policy maintains exploration without collapsing.</li>
</ul>
<h3 id="vehicle-speed-tracking_1">Vehicle Speed Tracking<a class="headerlink" href="#vehicle-speed-tracking_1" title="Permanent link">&para;</a></h3>
<p><img alt="Refactored PPO Inference" src="../assets/images/ppo_refactor_inference.png" /></p>
<p>The CAV tracks the head vehicle closely with minor overshoots (~1-2 m/s), comparable to the original implementation.</p>
<h2 id="key-takeaways">Key Takeaways<a class="headerlink" href="#key-takeaways" title="Permanent link">&para;</a></h2>
<ol>
<li>
<p><strong>Environment characteristics matter more than "best practices."</strong> CleanRL conventions are well-tested on standard benchmarks but don't automatically transfer to environments with discontinuous reward functions and hard safety constraints.</p>
</li>
<li>
<p><strong>Tanh squashing provides implicit regularization.</strong> Beyond bounding actions, tanh's saturating gradient naturally discourages extreme outputs. This is critical in environments where extreme actions have catastrophic consequences (collisions).</p>
</li>
<li>
<p><strong>Reward normalization interacts with reward discontinuities.</strong> <code>NormalizeReward</code> works best when the reward distribution is relatively smooth. Large outlier penalties (like the <span class="arithmatex">\(-100\)</span> safety penalty) can destabilize the running statistics and corrupt the learning signal.</p>
</li>
<li>
<p><strong>Hybrid approaches can capture the best of both worlds.</strong> Orthogonal initialization and tanh activations from CleanRL improved training, while the original tanh squashing and log-std clamping provided the stability needed for this specific environment.</p>
</li>
</ol>
<h2 id="hyperparameter-tuning">Hyperparameter Tuning<a class="headerlink" href="#hyperparameter-tuning" title="Permanent link">&para;</a></h2>
<p>PPO hyperparameters can be overridden via Hydra on the command line:</p>
<div class="highlight"><pre><span></span><code>uv<span class="w"> </span>run<span class="w"> </span>-m<span class="w"> </span>rl_mixed_traffic.ppo_train<span class="w"> </span>agent.lr<span class="o">=</span>1e-4<span class="w"> </span>agent.k_epochs<span class="o">=</span><span class="m">6</span><span class="w"> </span><span class="nv">total_steps</span><span class="o">=</span><span class="m">800000</span>
</code></pre></div>
<p>Key parameters to tune for this environment:</p>
<table>
<thead>
<tr>
<th>Parameter</th>
<th>Default</th>
<th>Effect</th>
</tr>
</thead>
<tbody>
<tr>
<td><code>agent.lr</code></td>
<td>3e-4</td>
<td>Lower (1e-4) for smoother policy updates</td>
</tr>
<tr>
<td><code>agent.k_epochs</code></td>
<td>10</td>
<td>Lower (4-6) to prevent overfitting per rollout</td>
</tr>
<tr>
<td><code>agent.clip_epsilon</code></td>
<td>0.2</td>
<td>Lower (0.1-0.15) for tighter trust region</td>
</tr>
<tr>
<td><code>rollout_steps</code></td>
<td>2048</td>
<td>Higher (4096) for more stable advantage estimates</td>
</tr>
<tr>
<td><code>agent.batch_size</code></td>
<td>64</td>
<td>Higher (128-256) to reduce gradient noise</td>
</tr>
<tr>
<td><code>total_steps</code></td>
<td>600,000</td>
<td>Higher (800k-1M) for more training time</td>
</tr>
</tbody>
</table>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      
      <script id="__config" type="application/json">{"annotate": null, "base": "..", "features": ["navigation.sections", "navigation.expand", "content.code.copy"], "search": "../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.79ae519e.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>