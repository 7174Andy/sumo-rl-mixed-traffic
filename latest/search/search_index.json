{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SUMO RL Mixed Traffic","text":"<p>Reinforcement learning for traffic flow optimization using SUMO (Simulation of Urban Mobility).</p>"},{"location":"#overview","title":"Overview","text":"<p>This project implements RL agents that control a single autonomous vehicle in a ring road scenario to maximize traffic flow by learning optimal speed control policies.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13+</li> <li>SUMO installed with <code>SUMO_HOME</code> environment variable set</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>uv sync\n</code></pre>"},{"location":"#training","title":"Training","text":"<p>Q-Learning:</p> <pre><code>uv run rl_mixed_traffic/q_train.py\n</code></pre> <p>DQN:</p> <pre><code>uv run rl_mixed_traffic/dqn_train.py\n</code></pre>"},{"location":"#evaluation","title":"Evaluation","text":"<p>Q-Learning:</p> <pre><code>uv run rl_mixed_traffic/q_eval_policy.py\n</code></pre> <p>DQN:</p> <pre><code>uv run rl_mixed_traffic/dqn_eval.py\n</code></pre>"},{"location":"algorithms/","title":"Algorithms","text":""},{"location":"algorithms/#1-tabular-q-learning","title":"1. Tabular Q-Learning","text":""},{"location":"algorithms/#11-overview","title":"1.1 Overview","text":""},{"location":"algorithms/#12-state-discretization","title":"1.2 State Discretization","text":""},{"location":"algorithms/#13-update-rule","title":"1.3 Update Rule","text":""},{"location":"algorithms/#14-exploration-strategy","title":"1.4 Exploration Strategy","text":""},{"location":"algorithms/#15-hyperparameters","title":"1.5 Hyperparameters","text":""},{"location":"algorithms/#2-deep-q-network-dqn","title":"2. Deep Q-Network (DQN)","text":""},{"location":"algorithms/#21-overview","title":"2.1 Overview","text":""},{"location":"algorithms/#22-network-architecture","title":"2.2 Network Architecture","text":""},{"location":"algorithms/#23-double-dqn","title":"2.3 Double DQN","text":""},{"location":"algorithms/#24-experience-replay","title":"2.4 Experience Replay","text":""},{"location":"algorithms/#25-target-network","title":"2.5 Target Network","text":""},{"location":"algorithms/#26-training-details","title":"2.6 Training Details","text":""},{"location":"algorithms/#27-hyperparameters","title":"2.7 Hyperparameters","text":""},{"location":"algorithms/#references","title":"References","text":""},{"location":"problem/","title":"Problem Formulation","text":""},{"location":"problem/#11-environment-ring-road-mixed-traffic","title":"1.1 Environment: Ring Road Mixed Traffic","text":"<p>This project studies a single-lane ring road where one RL-controlled autonomous vehicle (CAV) shares the road with a human-driven head vehicle. The ring topology eliminates boundary effects (on-ramps, traffic lights) and isolates the core car-following dynamics.</p> <p>The head vehicle (<code>car0</code>) periodically changes its cruising speed at random, creating disturbances that propagate through the ring. The AV (<code>car1</code>) must learn a speed-control policy that maintains safe following distances, tracks the traffic flow, and avoids abrupt acceleration changes.</p> <p>The problem is formulated as a Markov Decision Process (MDP):</p> Component Description State Normalized velocities and positions of all vehicles on the ring Action Longitudinal acceleration command applied to the CAV Reward Quadratic cost penalizing velocity error, spacing error, and control effort Transition Determined by SUMO's car-following models and the applied acceleration <p>The simulation runs at a time step of \\(\\Delta t = 0.1\\,\\text{s}\\), and each episode lasts up to 500 s (5,000 steps).</p>"},{"location":"problem/#12-state-space","title":"1.2 State Space","text":"<p>The observation is a fixed-length vector of normalized velocities and positions for all \\(N\\) vehicles:</p> \\[ \\mathbf{s} = \\bigl[\\underbrace{v_0/v_{\\max},\\;\\dots,\\;v_{N-1}/v_{\\max}}_{\\text{velocities}},\\;\\underbrace{p_0/L,\\;\\dots,\\;p_{N-1}/L}_{\\text{positions}}\\bigr] \\] <p>where:</p> <ul> <li>\\(v_{\\max} = 30\\;\\text{m/s}\\) is the maximum speed.</li> <li>\\(L\\) is the ring circumference (computed from the SUMO network at startup).</li> <li>Vehicles are sorted by ID so each index always maps to the same vehicle.</li> <li>If fewer than \\(N\\) vehicles are present, the vector is zero-padded to maintain a fixed shape of \\((2N,)\\).</li> <li>All values are clipped to \\([0, 1]\\).</li> </ul> <p>For the default two-vehicle scenario, the observation has shape \\((4,)\\): <code>[v_head, v_agent, p_head, p_agent]</code>.</p>"},{"location":"problem/#13-action-space","title":"1.3 Action Space","text":"<p>The native action space is a continuous scalar acceleration:</p> \\[ a \\in [a_{\\min},\\; a_{\\max}] = [-3.0,\\; 3.0]\\;\\text{m/s}^2 \\] <p>The acceleration is integrated into velocity each step:</p> \\[ v_{t+1} = \\text{clip}\\bigl(v_t + a \\cdot \\Delta t,\\; 0,\\; v_{\\max}\\bigr) \\] <p>and applied to the vehicle via TraCI's <code>setSpeed()</code> for immediate response.</p>"},{"location":"problem/#discretization","title":"Discretization","text":"<p>Because both Q-learning and DQN operate over discrete actions, the continuous range is discretized into \\(n\\) evenly spaced bins using <code>DiscretizeActionWrapper</code>:</p> \\[ \\mathcal{A} = \\bigl\\{a_{\\min},\\;\\; a_{\\min} + \\delta,\\;\\; a_{\\min} + 2\\delta,\\;\\;\\dots,\\;\\; a_{\\max}\\bigr\\}, \\quad \\delta = \\frac{a_{\\max} - a_{\\min}}{n - 1} \\]"},{"location":"problem/#14-reward-function","title":"1.4 Reward Function","text":"<p>The active reward function is based on the DeeP-LCC [1] cost formulation, which penalizes deviations from equilibrium velocity, desired spacing, and excessive control effort.</p>"},{"location":"problem/#components","title":"Components","text":"<p>Velocity error \u2014 penalizes deviation from the target speed (\\(v^{*} = 15\\;\\text{m/s}\\)):</p> \\[ R_v = -w_v \\cdot (v_{\\text{ego}} - v^{*})^2 \\] <p>Spacing error \u2014 penalizes deviation from the OVM equilibrium spacing \\(s^{*}\\):</p> \\[ R_s = -w_s \\cdot \\bigl(\\text{clip}(d_{\\text{gap}} - s^{*},\\;-20,\\;20)\\bigr)^2 \\] <p>where \\(s^{*}\\) is computed using the Optimal Velocity Model (OVM):</p> \\[ s^{*} = \\frac{\\arccos\\!\\bigl(1 - 2\\,v^{*}/v_{\\max}\\bigr)}{\\pi}\\,(s_{\\text{go}} - s_{\\text{st}}) + s_{\\text{st}} \\] <p>with \\(s_{\\text{st}} = 5\\;\\text{m}\\) (stop spacing) and \\(s_{\\text{go}} = 35\\;\\text{m}\\) (free-flow spacing).</p> <p>Control effort \u2014 penalizes large accelerations for smooth driving:</p> \\[ R_u = -w_u \\cdot a^2 \\] <p>Safety constraint \u2014 a hard penalty when the gap falls below a minimum:</p> \\[ R_{\\text{safety}} = \\begin{cases} -100 &amp; \\text{if } d_{\\text{gap}} &lt; s_{\\min} \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\]"},{"location":"problem/#total-reward","title":"Total Reward","text":"\\[ R = \\frac{R_v + R_s + R_u + R_{\\text{safety}}}{100} \\] <p>The division by 100 scales per-step values into a range suitable for policy gradient methods.</p>"},{"location":"problem/#default-weights","title":"Default Weights","text":"Parameter Symbol Default Velocity \\(w_v\\) 0.8 Spacing \\(w_s\\) 0.7 Control \\(w_u\\) 0.1 Min gap \\(s_{\\min}\\) 5.0 m"},{"location":"problem/#references","title":"References","text":"<p>[1] Wang, J., Zheng, Y., Li, K., &amp; Xu, Q. (2023). DeeP-LCC: Data-EnablEd Predictive Leading Cruise Control in Mixed Traffic Flow. IEEE Transactions on Control Systems Technology, 31(6), 2760\u20132776. doi:10.1109/tcst.2023.3288636</p>"}]}