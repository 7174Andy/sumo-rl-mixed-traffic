{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SUMO RL Mixed Traffic","text":"<p>Reinforcement learning for traffic flow optimization using SUMO (Simulation of Urban Mobility).</p>"},{"location":"#overview","title":"Overview","text":"<p>This project implements RL agents that control a single autonomous vehicle in a ring road scenario to maximize traffic flow by learning optimal speed control policies.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13+</li> <li>SUMO installed with <code>SUMO_HOME</code> environment variable set</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>uv sync\n</code></pre>"},{"location":"#training","title":"Training","text":"<p>Q-Learning:</p> <pre><code>uv run rl_mixed_traffic/q_train.py\n</code></pre> <p>DQN:</p> <pre><code>uv run rl_mixed_traffic/dqn_train.py\n</code></pre>"},{"location":"#evaluation","title":"Evaluation","text":"<p>Q-Learning:</p> <pre><code>uv run rl_mixed_traffic/q_eval_policy.py\n</code></pre> <p>DQN:</p> <pre><code>uv run rl_mixed_traffic/dqn_eval.py\n</code></pre>"},{"location":"algorithms/","title":"Algorithms","text":""},{"location":"algorithms/#1-problem-formulation","title":"1. Problem Formulation","text":""},{"location":"algorithms/#11-environment-ring-road-mixed-traffic","title":"1.1 Environment: Ring Road Mixed Traffic","text":""},{"location":"algorithms/#12-state-space","title":"1.2 State Space","text":""},{"location":"algorithms/#13-action-space","title":"1.3 Action Space","text":""},{"location":"algorithms/#14-reward-function","title":"1.4 Reward Function","text":""},{"location":"algorithms/#2-tabular-q-learning","title":"2. Tabular Q-Learning","text":""},{"location":"algorithms/#21-overview","title":"2.1 Overview","text":""},{"location":"algorithms/#22-state-discretization","title":"2.2 State Discretization","text":""},{"location":"algorithms/#23-update-rule","title":"2.3 Update Rule","text":""},{"location":"algorithms/#24-exploration-strategy","title":"2.4 Exploration Strategy","text":""},{"location":"algorithms/#25-hyperparameters","title":"2.5 Hyperparameters","text":""},{"location":"algorithms/#3-deep-q-network-dqn","title":"3. Deep Q-Network (DQN)","text":""},{"location":"algorithms/#31-overview","title":"3.1 Overview","text":""},{"location":"algorithms/#32-network-architecture","title":"3.2 Network Architecture","text":""},{"location":"algorithms/#33-double-dqn","title":"3.3 Double DQN","text":""},{"location":"algorithms/#34-experience-replay","title":"3.4 Experience Replay","text":""},{"location":"algorithms/#35-target-network","title":"3.5 Target Network","text":""},{"location":"algorithms/#36-training-details","title":"3.6 Training Details","text":""},{"location":"algorithms/#37-hyperparameters","title":"3.7 Hyperparameters","text":""},{"location":"algorithms/#4-sumo-integration-details","title":"4. SUMO Integration Details","text":""},{"location":"algorithms/#41-traci-control","title":"4.1 TraCI Control","text":""},{"location":"algorithms/#42-head-vehicle-behavior","title":"4.2 Head Vehicle Behavior","text":""},{"location":"algorithms/#43-leader-detection","title":"4.3 Leader Detection","text":""},{"location":"algorithms/#references","title":"References","text":""}]}