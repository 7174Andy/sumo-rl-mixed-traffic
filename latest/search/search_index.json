{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SUMO RL Mixed Traffic","text":"<p>Reinforcement learning for traffic flow optimization using SUMO (Simulation of Urban Mobility).</p>"},{"location":"#overview","title":"Overview","text":"<p>This project implements RL agents that control a single autonomous vehicle in a ring road scenario to maximize traffic flow by learning optimal speed control policies.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13+</li> <li>SUMO installed with <code>SUMO_HOME</code> environment variable set</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>uv sync\n</code></pre>"},{"location":"#training","title":"Training","text":"<p>Q-Learning:</p> <pre><code>uv run rl_mixed_traffic/q_train.py\n</code></pre> <p>DQN:</p> <pre><code>uv run rl_mixed_traffic/dqn_train.py\n</code></pre>"},{"location":"#evaluation","title":"Evaluation","text":"<p>Q-Learning:</p> <pre><code>uv run rl_mixed_traffic/q_eval_policy.py\n</code></pre> <p>DQN:</p> <pre><code>uv run rl_mixed_traffic/dqn_eval.py\n</code></pre>"},{"location":"algorithms/","title":"Algorithms","text":""},{"location":"algorithms/#1-tabular-q-learning","title":"1. Tabular Q-Learning","text":""},{"location":"algorithms/#11-overview","title":"1.1 Overview","text":"<p>Q-learning is a model-free, off-policy reinforcement learning algorithm that learns an action-value function \\(Q(s, a)\\) mapping state-action pairs to expected cumulative reward. It is \"tabular\" because \\(Q\\) is stored as a lookup table (implemented as a <code>defaultdict</code>) rather than approximated by a neural network.</p> <p>Tabular Q-learning is a natural first baseline for this problem because the discretized state and action spaces are small enough to enumerate, updates are simple and stable, and there are no neural-network hyperparameters to tune.</p>"},{"location":"algorithms/#12-state-discretization","title":"1.2 State Discretization","text":"<p>The continuous observation vector \\(\\mathbf{s} \\in [0, 1]^{2N}\\) must be converted to a discrete key for table lookup. The <code>StateDiscretizer</code> partitions each dimension into \\(B\\) bins using edge arrays and maps each continuous value to a bin index via <code>np.digitize</code>. The result is an integer tuple of length \\(2N\\):</p> \\[ \\mathbf{s}_{\\text{disc}} = \\bigl(b_0,\\; b_1,\\; \\dots,\\; b_{2N-1}\\bigr), \\quad b_i \\in \\{0, \\dots, B-1\\} \\] <p>Two binning modes are available:</p> Mode Dimensions Spacing Use case Uniform (default) All Equal-width bins across \\([0, 1]\\) General purpose Position heavy-tail Position dims only Quadratic spacing (\\(u^2\\)) concentrating bins near 0 Higher resolution at small headways <p>In the default training configuration, \\(B = 20\\) bins per dimension are used for both velocity and position dimensions.</p>"},{"location":"algorithms/#13-update-rule","title":"1.3 Update Rule","text":"<p>After each transition \\((s, a, r, s')\\), the Q-value is updated using the standard off-policy TD(0) rule:</p> \\[ Q(s, a) \\leftarrow (1 - \\alpha)\\,Q(s, a) + \\alpha\\,\\bigl[r + \\gamma \\max_{a'} Q(s', a')\\bigr] \\] <p>where:</p> <ul> <li>\\(\\alpha\\) is the learning rate controlling how much new information overrides the old estimate.</li> <li>\\(\\gamma\\) is the discount factor weighting future rewards.</li> <li>\\(\\max_{a'} Q(s', a')\\) is the best estimated value from the next state (set to 0 at terminal states).</li> </ul> <p>This is equivalent to the more commonly written form:</p> \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha\\,\\bigl[r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)\\bigr] \\]"},{"location":"algorithms/#14-exploration-strategy","title":"1.4 Exploration Strategy","text":"<p>The agent uses an \\(\\varepsilon\\)-greedy policy with linear decay:</p> \\[ \\varepsilon(t) = \\varepsilon_{\\text{start}} + \\frac{t}{T_{\\text{decay}}} \\cdot (\\varepsilon_{\\text{end}} - \\varepsilon_{\\text{start}}) \\] <p>where \\(t\\) is the current step count and \\(T_{\\text{decay}}\\) is the total decay horizon. At each step:</p> <ul> <li>With probability \\(\\varepsilon\\), the agent selects a random action (exploration).</li> <li>With probability \\(1 - \\varepsilon\\), the agent selects \\(\\arg\\max_a Q(s, a)\\) (exploitation), breaking ties randomly.</li> </ul> <p>Epsilon starts at 1.0 (fully random) and decays linearly to 0.05 over the full training run, encouraging broad exploration early and convergence to the learned policy later.</p>"},{"location":"algorithms/#15-hyperparameters","title":"1.5 Hyperparameters","text":"Parameter Symbol Default Learning rate \\(\\alpha\\) 0.2 Discount factor \\(\\gamma\\) 0.98 Initial epsilon \\(\\varepsilon_{\\text{start}}\\) 1.0 Final epsilon \\(\\varepsilon_{\\text{end}}\\) 0.05 Epsilon decay steps \\(T_{\\text{decay}}\\) <code>num_episodes * max_steps</code> Action bins \\(n\\) 20 State bins per dim \\(B\\) 20 Episodes \u2014 250"},{"location":"algorithms/#16-problems-and-limitations","title":"1.6 Problems and Limitations","text":"<p>For this specific problem of controlling CAV, tabular Q-learning algorithm has several limitations:</p> <ol> <li> <p>Scalability: The state-action space grows exponentially with the number of vehicles and discretization bins, leading to a large Q-table that may not fit in memory or converge within a reasonable time.</p> </li> <li> <p>Generalization: The agent cannot generalize to unseen states or actions, which may occur frequently in a continuous environment.</p> </li> <li> <p>Loss of precision from discretization: The discretization process may lose important information about the state, especially if the bins are too coarse. This can lead to suboptimal policies.</p> </li> </ol> <p>Therefore, we concluded to use a Deep Q-Network (DQN) that can handle continuous state spaces and generalize better, while still being sample efficient and stable for training.</p>"},{"location":"algorithms/#2-deep-q-network-dqn","title":"2. Deep Q-Network (DQN)","text":""},{"location":"algorithms/#21-overview","title":"2.1 Overview","text":"<p>Deep Q-Networks (DQN) replace the Q-table with a neural network \\(Q_\\theta(s, a)\\) that maps continuous state vectors directly to action values. This addresses the key limitations of tabular Q-learning identified above: the network generalizes across similar states without requiring explicit discretization, and its parameter count is fixed regardless of the state space size.</p> <p>The implementation uses Double DQN [1] with a separate target network and experience replay buffer for stable training.</p>"},{"location":"algorithms/#22-network-architecture","title":"2.2 Network Architecture","text":"<p>The Q-network is a two-layer MLP that maps the observation vector to Q-values for each discrete action:</p> \\[ \\mathbf{s} \\in \\mathbb{R}^{2N} \\xrightarrow{\\text{Linear}(2N, 128)} \\xrightarrow{\\text{ReLU}} \\xrightarrow{\\text{Linear}(128, |\\mathcal{A}|)} \\mathbf{q} \\in \\mathbb{R}^{|\\mathcal{A}|} \\] Layer Input dim Output dim Activation <code>fc1</code> \\(2N\\) 128 ReLU <code>out</code> 128 \\(\\lvert\\mathcal{A}\\rvert\\) None (linear) <p>For the default 4-vehicle, 21-action configuration: input dim = 8, output dim = 21, total parameters \\(\\approx\\) 3,900.</p>"},{"location":"algorithms/#23-double-dqn","title":"2.3 Double DQN","text":"<p>Standard DQN uses the same network to both select and evaluate the next-state action, which leads to systematic overestimation of Q-values. Double DQN decouples these two steps:</p> <ol> <li>The online network \\(Q_\\theta\\) selects the best next action:</li> </ol> \\[ a^* = \\arg\\max_{a'} Q_\\theta(s', a') \\] <ol> <li>The target network \\(Q_{\\theta^-}\\) evaluates that action:</li> </ol> \\[ y = r + \\gamma\\,(1 - d)\\,Q_{\\theta^-}(s', a^*) \\] <p>where \\(d = 1\\) if the episode terminated, \\(d = 0\\) otherwise. This reduces overestimation bias by preventing the same network from both proposing and scoring the greedy action.</p>"},{"location":"algorithms/#24-experience-replay","title":"2.4 Experience Replay","text":"<p>Transitions \\((s, a, r, s', d)\\) are stored in a fixed-capacity replay buffer implemented as a <code>deque</code> with maximum size 200,000. When the buffer is full, the oldest transitions are discarded.</p> <p>During each learning step, a uniform random batch of 128 transitions is sampled. This breaks temporal correlations between consecutive samples and allows each transition to be reused across multiple updates, improving sample efficiency.</p> <p>Learning begins only after the buffer accumulates at least 10,000 transitions (<code>start_learning_after</code>), ensuring the initial batches are sufficiently diverse.</p>"},{"location":"algorithms/#25-target-network","title":"2.5 Target Network","text":"<p>A separate target network \\(Q_{\\theta^-}\\) provides stable regression targets during training. It is updated every 3,000 steps using soft updates (Polyak averaging):</p> \\[ \\theta^- \\leftarrow (1 - \\tau)\\,\\theta^- + \\tau\\,\\theta \\] <p>with \\(\\tau = 0.01\\) by default. This gradually blends the online network's weights into the target, providing a smoother learning signal than periodic hard copies (\\(\\tau = 1.0\\)).</p> <p>When \\(\\tau = 1.0\\), the update becomes a hard copy of the online network weights.</p>"},{"location":"algorithms/#26-training-details","title":"2.6 Training Details","text":"<p>Loss function: Smooth L1 (Huber) loss between predicted and target Q-values:</p> \\[ \\mathcal{L} = \\text{SmoothL1}\\bigl(Q_\\theta(s, a),\\; y\\bigr) \\] <p>Huber loss behaves as MSE for small errors and as MAE for large errors, making it more robust to outlier transitions than pure MSE.</p> <p>Optimizer: Adam with learning rate \\(5 \\times 10^{-4}\\).</p> <p>Gradient clipping: Gradients are clipped by global norm to 10.0 via <code>clip_grad_norm_</code> to prevent destabilizing parameter updates.</p> <p>Training loop: The agent operates in a step-based loop (not episode-based). At each of the 350,000 total steps:</p> <ol> <li>Select action via \\(\\varepsilon\\)-greedy (same linear decay as Q-learning).</li> <li>Execute action in SUMO, observe \\((r, s', d)\\).</li> <li>Store the transition in the replay buffer.</li> <li>If buffer size \\(\\geq\\) 10,000 and <code>steps % train_freq == 0</code>: sample a batch and perform one gradient step.</li> <li>If <code>steps % target_update_freq == 0</code>: update the target network.</li> <li>On episode termination, reset the environment and log the return.</li> </ol>"},{"location":"algorithms/#27-hyperparameters","title":"2.7 Hyperparameters","text":"Parameter Symbol Default Learning rate \\(\\eta\\) \\(5 \\times 10^{-4}\\) Discount factor \\(\\gamma\\) 0.99 Batch size \u2014 128 Buffer size \u2014 200,000 Start learning after \u2014 10,000 steps Train frequency \u2014 Every 1 step Target update freq \u2014 Every 3,000 steps Soft update factor \\(\\tau\\) 0.01 Initial epsilon \\(\\varepsilon_{\\text{start}}\\) 1.0 Final epsilon \\(\\varepsilon_{\\text{end}}\\) 0.10 Epsilon decay steps \\(T_{\\text{decay}}\\) 500,000 Max gradient norm \u2014 10.0 Action bins \\(\\lvert\\mathcal{A}\\rvert\\) 21 Total training steps \u2014 350,000"},{"location":"algorithms/#28-results","title":"2.8 Results","text":""},{"location":"algorithms/#29-problems-and-limitations","title":"2.9 Problems and Limitations","text":"<p>Still, the action space is discretized, which may limit the optimality of the learned policy. The agent cannot output fine-grained accelerations, which may be necessary for smooth control in a continuous environment. As shown in the DQN performance plot, the agent learns a reasonable policy but displays some spikes in the velocity to catch up the head vehicle, which may not be ideal for smooth traffic flow. Therefore, we concluded to use a policy-gradient method that can directly output continuous actions, such as Proximal Policy Optimization (PPO), which is more suitable for continuous control tasks and can learn more stable policies with fewer hyperparameters to tune.</p>"},{"location":"algorithms/#3-proximal-policy-optimization-ppo","title":"3. Proximal Policy Optimization (PPO)","text":""},{"location":"algorithms/#31-overview","title":"3.1 Overview","text":"<p>PPO is an on-policy, policy-gradient algorithm that directly optimizes a parameterized policy \\(\\pi_\\theta(a \\mid s)\\) rather than learning action values. Unlike Q-learning and DQN, PPO naturally handles continuous action spaces \u2014 the policy outputs a Gaussian distribution over accelerations, eliminating the need for action discretization and enabling fine-grained, smooth control.</p> <p>The implementation uses an Actor-Critic architecture with a clipped surrogate objective and Generalized Advantage Estimation (GAE) [2].</p>"},{"location":"algorithms/#32-network-architecture","title":"3.2 Network Architecture","text":"<p>The Actor-Critic network shares feature extraction layers between the policy (actor) and the value function (critic):</p> \\[ \\mathbf{s} \\in \\mathbb{R}^{2N} \\xrightarrow{\\text{Linear}(2N, 256)} \\xrightarrow{\\tanh} \\xrightarrow{\\text{Linear}(256, 256)} \\xrightarrow{\\tanh} \\begin{cases} \\xrightarrow{\\text{Actor head}} \\mu \\in \\mathbb{R}^{d_a} \\\\ \\xrightarrow{\\text{Critic head}} V \\in \\mathbb{R} \\end{cases} \\] Layer Input dim Output dim Activation Init <code>shared1</code> \\(2N\\) 256 tanh Orthogonal (std=\\(\\sqrt{2}\\)) <code>shared2</code> 256 256 tanh Orthogonal (std=\\(\\sqrt{2}\\)) <code>actor_mean</code> 256 \\(d_a\\) None Orthogonal (std=0.01) <code>critic_head</code> 256 1 None Orthogonal (std=1.0) <p>Gaussian policy: The actor outputs a mean \\(\\mu_\\theta(s)\\) and uses a learnable, state-independent log standard deviation \\(\\log \\sigma\\) (clamped to \\([-2.0, 0.5]\\)). Actions are sampled from \\(\\mathcal{N}(\\mu, \\sigma^2)\\) and squashed through \\(\\tanh\\) to produce bounded outputs in \\([-1, 1]\\), which are then rescaled to the acceleration range \\([-3, 3]\\) m/s\u00b2.</p> <p>The log-probability is corrected for the tanh change of variables:</p> \\[ \\log \\pi(a \\mid s) = \\log \\mu(u \\mid s) - \\sum_i \\log(1 - \\tanh^2(u_i)) \\] <p>where \\(u\\) is the pre-squashed sample.</p>"},{"location":"algorithms/#33-clipped-surrogate-objective","title":"3.3 Clipped Surrogate Objective","text":"<p>PPO constrains policy updates using a clipped probability ratio to prevent destructively large steps:</p> \\[ L^{\\text{CLIP}} = \\mathbb{E}\\Bigl[\\min\\bigl(r_t \\hat{A}_t,\\;\\; \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon)\\,\\hat{A}_t\\bigr)\\Bigr] \\] <p>where:</p> <ul> <li>\\(r_t = \\frac{\\pi_\\theta(a_t \\mid s_t)}{\\pi_{\\theta_\\text{old}}(a_t \\mid s_t)}\\) is the probability ratio between new and old policies.</li> <li>\\(\\hat{A}_t\\) is the estimated advantage (see Section 3.4).</li> <li>\\(\\epsilon = 0.2\\) is the clipping threshold.</li> </ul> <p>The \\(\\min\\) ensures that when the advantage is positive, the ratio cannot exceed \\(1 + \\epsilon\\), and when negative, it cannot fall below \\(1 - \\epsilon\\). This creates a trust region that keeps the updated policy close to the data-collecting policy.</p>"},{"location":"algorithms/#34-generalized-advantage-estimation-gae","title":"3.4 Generalized Advantage Estimation (GAE)","text":"<p>Advantages are computed using GAE, which provides a tunable bias-variance tradeoff via the \\(\\lambda\\) parameter:</p> \\[ \\hat{A}_t = \\sum_{l=0}^{T-t-1} (\\gamma \\lambda)^l \\delta_{t+l} \\] <p>where the TD residual is:</p> \\[ \\delta_t = r_t + \\gamma\\,(1 - d_t)\\,V(s_{t+1}) - V(s_t) \\] <ul> <li>When \\(\\lambda = 0\\): pure one-step TD (low variance, high bias).</li> <li>When \\(\\lambda = 1\\): Monte Carlo return (high variance, low bias).</li> <li>Default \\(\\lambda = 0.95\\) balances both.</li> </ul> <p>Returns for the value function target are computed as \\(R_t = \\hat{A}_t + V(s_t)\\).</p>"},{"location":"algorithms/#35-training-loop","title":"3.5 Training Loop","text":"<p>PPO uses a rollout-based training loop that alternates between data collection and policy updates:</p> <ol> <li>Collect rollout: Run the current policy for 2,048 steps, storing \\((s, a, r, V(s), \\log\\pi(a|s), d)\\) in the rollout buffer.</li> <li>Compute advantages: Apply GAE over the collected rollout, bootstrapping from \\(V(s_T)\\) if the episode did not terminate.</li> <li>Normalize advantages: Subtract mean and divide by standard deviation for training stability.</li> <li>Minibatch updates: For \\(K = 10\\) epochs, shuffle the rollout data and iterate over minibatches of size 64:</li> <li>Recompute \\(\\log\\pi_\\theta(a|s)\\) and \\(V_\\theta(s)\\) under the current parameters.</li> <li>Compute the clipped policy loss, value loss (MSE), and entropy bonus.</li> <li>Minimize the combined loss: \\(L = L^{\\text{CLIP}} + c_v L^{\\text{value}} + c_e L^{\\text{entropy}}\\).</li> <li>Clip gradients by global norm to 0.5.</li> <li>Clear buffer and repeat from step 1.</li> </ol>"},{"location":"algorithms/#36-loss-function","title":"3.6 Loss Function","text":"<p>The total loss combines three terms:</p> \\[ L = L^{\\text{CLIP}} + c_v \\cdot \\text{MSE}\\bigl(V_\\theta(s),\\; R_t\\bigr) - c_e \\cdot H[\\pi_\\theta(\\cdot \\mid s)] \\] Term Coefficient Purpose Policy loss 1.0 Maximize clipped advantage Value loss \\(c_v = 0.5\\) Fit value function to returns Entropy bonus \\(c_e = 0.01\\) Encourage exploration, prevent collapse"},{"location":"algorithms/#37-hyperparameters","title":"3.7 Hyperparameters","text":"Parameter Symbol Default Learning rate \\(\\eta\\) \\(3 \\times 10^{-4}\\) Discount factor \\(\\gamma\\) 0.99 GAE lambda \\(\\lambda\\) 0.95 Clip epsilon \\(\\epsilon\\) 0.2 Epochs per update \\(K\\) 10 Minibatch size \u2014 64 Value coefficient \\(c_v\\) 0.5 Entropy coefficient \\(c_e\\) 0.01 Max gradient norm \u2014 0.5 Rollout steps \u2014 2,048 Total training steps \u2014 600,000 Hidden dim \u2014 256"},{"location":"algorithms/#38-results","title":"3.8 Results","text":""},{"location":"algorithms/#references","title":"References","text":"<p>[1] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning. arXiv [Cs.LG]. Retrieved from http://arxiv.org/abs/1312.5602</p> <p>[2] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). Proximal Policy Optimization Algorithms. arXiv [Cs.LG]. Retrieved from http://arxiv.org/abs/1707.06347</p>"},{"location":"ppo-refactor/","title":"PPO Refactoring: From CleanRL-Style to Stable Training","text":"<p>This page documents the refactoring of the PPO implementation, the problems encountered, and the lessons learned about continuous action spaces in traffic control environments.</p>"},{"location":"ppo-refactor/#motivation","title":"Motivation","text":"<p>The original PPO implementation used tanh-squashed Gaussian policies with manual action scaling and a hardcoded reward divisor (<code>R /= 100</code>). While functional, the code had several issues:</p> <ul> <li>Tanh squashing required a manual Jacobian correction for log-probabilities</li> <li>Action scaling from \\([-1, 1]\\) to \\([-3, 3]\\) was done outside the environment</li> <li>The reward divisor was a magic constant with no adaptive behavior</li> </ul> <p>The goal was to modernize the PPO code following CleanRL conventions, which have been shown to improve training stability and reproducibility across standard benchmarks (MuJoCo, Atari, etc.).</p>"},{"location":"ppo-refactor/#the-cleanrl-style-refactoring","title":"The CleanRL-Style Refactoring","text":"<p>The refactoring applied the following changes:</p>"},{"location":"ppo-refactor/#changes-made","title":"Changes Made","text":"Component Before After (CleanRL) Action output <code>tanh(sample)</code>, manual scale to \\([-3, 3]\\) Raw Gaussian sample, <code>ClipAction</code> wrapper Log-probability Gaussian log-prob with Jacobian correction Plain Gaussian log-prob Hidden activations ReLU tanh Weight initialization PyTorch defaults Orthogonal init (std=\\(\\sqrt{2}\\) for shared, 0.01 for actor, 1.0 for critic) Log-std clamping Clamped to \\([-2.0, 0.5]\\) Unclamped Reward handling <code>R /= 100</code> hardcoded <code>NormalizeReward</code> + <code>TransformReward(clip to [-10, 10])</code> wrappers Step API 4-tuple <code>(obs, reward, done, info)</code> 5-tuple via <code>FourToFiveTupleWrapper</code>"},{"location":"ppo-refactor/#wrapper-chain","title":"Wrapper Chain","text":"<p>The refactored environment applies a chain of Gymnasium wrappers:</p> <pre><code>RingRoadEnv -&gt; FourToFiveTupleWrapper -&gt; ClipAction -&gt; NormalizeReward -&gt; TransformReward\n</code></pre> <ul> <li>FourToFiveTupleWrapper: Bridges the 4-tuple <code>step()</code> API to Gymnasium's 5-tuple <code>(obs, reward, terminated, truncated, info)</code> without modifying the base environment.</li> <li>ClipAction: Clips actions to the environment's action space bounds \\([-3, 3]\\).</li> <li>NormalizeReward: Divides rewards by a running standard deviation for adaptive scaling.</li> <li>TransformReward: Clips normalized rewards to \\([-10, 10]\\) to prevent outliers.</li> </ul>"},{"location":"ppo-refactor/#results-unstable-training","title":"Results: Unstable Training","text":"<p>The refactored version showed significantly worse training stability compared to the original.</p>"},{"location":"ppo-refactor/#training-returns-comparison","title":"Training Returns Comparison","text":"<p>Original (tanh squashing):</p> <p></p> <p>CleanRL-style (raw Gaussian + ClipAction):</p> <p>The training curve showed frequent collapses throughout the entire run. Episodes that had been stable would suddenly drop to returns 5-10x worse, and the moving average never fully stabilized.</p>"},{"location":"ppo-refactor/#vehicle-speed-tracking","title":"Vehicle Speed Tracking","text":"<p>Original:</p> <p></p> <p>CleanRL-style:</p> <p>The CAV exhibited large speed overshoots (5-6 m/s above the head vehicle) and oscillatory tracking behavior, indicating the learned policy was applying overly aggressive accelerations.</p>"},{"location":"ppo-refactor/#root-cause-analysis","title":"Root Cause Analysis","text":"<p>The instability was traced to a feedback loop between the raw Gaussian policy and the <code>NormalizeReward</code> wrapper:</p> <ol> <li> <p>Raw Gaussian outputs extreme actions. Without tanh's natural bounding, the policy can produce acceleration values that swing abruptly between the full \\(-3\\) and \\(+3\\) m/s\\(^2\\) range. The <code>ClipAction</code> wrapper hard-clips these, but provides zero gradient for out-of-bound outputs \u2014 the network cannot learn how far it overshot.</p> </li> <li> <p>Extreme actions cause collisions. Abrupt acceleration changes lead to unsafe following distances, triggering the \\(-100\\) safety penalty in the reward function.</p> </li> <li> <p>Safety penalties destabilize <code>NormalizeReward</code>. The large penalty shifts the running reward statistics, distorting the reward scale for subsequent episodes.</p> </li> <li> <p>Distorted rewards cause bad policy updates. The value function estimates become inaccurate, leading to poor advantage estimates and further policy degradation.</p> </li> </ol> <p>This cycle did not occur with tanh squashing because tanh naturally prevents extreme actions. The sigmoid-like shape means the network needs exponentially larger pre-activation values to push actions closer to the boundary, acting as an implicit regularizer.</p>"},{"location":"ppo-refactor/#why-cleanrl-conventions-didnt-transfer","title":"Why CleanRL Conventions Didn't Transfer","text":"<p>CleanRL's raw Gaussian approach works well for environments like MuJoCo (HalfCheetah, Hopper, etc.) because:</p> <ul> <li>Smooth dynamics: Physics simulators have continuous, differentiable dynamics. An extreme action produces a proportionally bad outcome, not a discontinuous penalty.</li> <li>No hard safety penalties: MuJoCo rewards are typically smooth quadratic costs. There is no equivalent of the \\(-100\\) collision penalty that can destabilize reward normalization.</li> <li>Dense reward signal: Every step provides informative gradient information. In the ring road environment, the reward signal is sparser and dominated by the safety constraint.</li> </ul> <p>The ring road environment has a discontinuous reward landscape (the \\(-100\\) safety penalty creates a cliff) that interacts poorly with reward normalization when the policy can produce extreme actions.</p>"},{"location":"ppo-refactor/#final-implementation-hybrid-approach","title":"Final Implementation: Hybrid Approach","text":"<p>The final implementation keeps the beneficial CleanRL changes while restoring tanh squashing:</p>"},{"location":"ppo-refactor/#what-was-kept-from-cleanrl","title":"What Was Kept from CleanRL","text":"<ul> <li>Orthogonal weight initialization with appropriate scales (std=\\(\\sqrt{2}\\) for shared layers, 0.01 for actor, 1.0 for critic)</li> <li>tanh hidden activations instead of ReLU in shared layers</li> <li><code>NormalizeReward</code> + <code>TransformReward</code> wrappers for adaptive reward scaling</li> <li><code>FourToFiveTupleWrapper</code> for Gymnasium 5-tuple API compatibility</li> </ul>"},{"location":"ppo-refactor/#what-was-reverted","title":"What Was Reverted","text":"<ul> <li>tanh action squashing restored \u2014 actions are squashed to \\([-1, 1]\\) then scaled by <code>max_accel</code></li> <li>Jacobian correction for log-probabilities restored:</li> </ul> \\[ \\log \\pi(a \\mid s) = \\log \\mu(u \\mid s) - \\sum_i \\log(1 - \\tanh^2(u_i)) \\] <ul> <li>Log-std clamping to \\([-2.0, 0.5]\\) restored to prevent exploration from becoming too large or too small</li> </ul>"},{"location":"ppo-refactor/#action-scaling","title":"Action Scaling","text":"<p>The tanh output \\(a_{\\tanh} \\in [-1, 1]\\) is scaled to the acceleration range using simple multiplication:</p> <pre><code>action_scaled = action_tanh * max_accel  # [-1, 1] * 3.0 -&gt; [-3, 3]\n</code></pre> <p>This works because the action space is symmetric. The <code>ClipAction</code> wrapper remains in the chain as an additional safety net.</p>"},{"location":"ppo-refactor/#results-stable-training","title":"Results: Stable Training","text":""},{"location":"ppo-refactor/#training-returns","title":"Training Returns","text":"<p>The training curve converges by episode ~85 and remains stable, with only rare dips. The MA(10) shows consistent improvement without the periodic collapses seen in the CleanRL-style version.</p>"},{"location":"ppo-refactor/#training-metrics","title":"Training Metrics","text":"<ul> <li>Policy loss: Near zero with isolated spikes that recover quickly.</li> <li>Value loss: Drops from 0.25 to near 0, stable after update 100.</li> <li>Entropy: Healthy fluctuation around 1.5 \u2014 the policy maintains exploration without collapsing.</li> </ul>"},{"location":"ppo-refactor/#vehicle-speed-tracking_1","title":"Vehicle Speed Tracking","text":"<p>The CAV tracks the head vehicle closely with minor overshoots (~1-2 m/s), comparable to the original implementation.</p>"},{"location":"ppo-refactor/#key-takeaways","title":"Key Takeaways","text":"<ol> <li> <p>Environment characteristics matter more than \"best practices.\" CleanRL conventions are well-tested on standard benchmarks but don't automatically transfer to environments with discontinuous reward functions and hard safety constraints.</p> </li> <li> <p>Tanh squashing provides implicit regularization. Beyond bounding actions, tanh's saturating gradient naturally discourages extreme outputs. This is critical in environments where extreme actions have catastrophic consequences (collisions).</p> </li> <li> <p>Reward normalization interacts with reward discontinuities. <code>NormalizeReward</code> works best when the reward distribution is relatively smooth. Large outlier penalties (like the \\(-100\\) safety penalty) can destabilize the running statistics and corrupt the learning signal.</p> </li> <li> <p>Hybrid approaches can capture the best of both worlds. Orthogonal initialization and tanh activations from CleanRL improved training, while the original tanh squashing and log-std clamping provided the stability needed for this specific environment.</p> </li> </ol>"},{"location":"ppo-refactor/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":"<p>PPO hyperparameters can be overridden via Hydra on the command line:</p> <pre><code>uv run -m rl_mixed_traffic.ppo_train agent.lr=1e-4 agent.k_epochs=6 total_steps=800000\n</code></pre> <p>Key parameters to tune for this environment:</p> Parameter Default Effect <code>agent.lr</code> 3e-4 Lower (1e-4) for smoother policy updates <code>agent.k_epochs</code> 10 Lower (4-6) to prevent overfitting per rollout <code>agent.clip_epsilon</code> 0.2 Lower (0.1-0.15) for tighter trust region <code>rollout_steps</code> 2048 Higher (4096) for more stable advantage estimates <code>agent.batch_size</code> 64 Higher (128-256) to reduce gradient noise <code>total_steps</code> 600,000 Higher (800k-1M) for more training time"},{"location":"problem/","title":"Problem Formulation","text":""},{"location":"problem/#11-environment-ring-road-mixed-traffic","title":"1.1 Environment: Ring Road Mixed Traffic","text":"<p>This project studies a single-lane ring road where one RL-controlled autonomous vehicle (CAV) shares the road with a human-driven head vehicle. The ring topology eliminates boundary effects (on-ramps, traffic lights) and isolates the core car-following dynamics.</p> <p>The head vehicle (<code>car0</code>) periodically changes its cruising speed at random, creating disturbances that propagate through the ring. The AV (<code>car1</code>) must learn a speed-control policy that maintains safe following distances, tracks the traffic flow, and avoids abrupt acceleration changes.</p> <p>The problem is formulated as a Markov Decision Process (MDP):</p> Component Description State Normalized velocities and positions of all vehicles on the ring Action Longitudinal acceleration command applied to the CAV Reward Quadratic cost penalizing velocity error, spacing error, and control effort Transition Determined by SUMO's car-following models and the applied acceleration <p>The simulation runs at a time step of \\(\\Delta t = 0.1\\,\\text{s}\\), and each episode lasts up to 500 s (5,000 steps).</p>"},{"location":"problem/#12-state-space","title":"1.2 State Space","text":"<p>The observation is a fixed-length vector of normalized velocities and positions for all \\(N\\) vehicles:</p> \\[ \\mathbf{s} = \\bigl[\\underbrace{v_0/v_{\\max},\\;\\dots,\\;v_{N-1}/v_{\\max}}_{\\text{velocities}},\\;\\underbrace{p_0/L,\\;\\dots,\\;p_{N-1}/L}_{\\text{positions}}\\bigr] \\] <p>where:</p> <ul> <li>\\(v_{\\max} = 30\\;\\text{m/s}\\) is the maximum speed.</li> <li>\\(L\\) is the ring circumference (computed from the SUMO network at startup).</li> <li>Vehicles are sorted by ID so each index always maps to the same vehicle.</li> <li>If fewer than \\(N\\) vehicles are present, the vector is zero-padded to maintain a fixed shape of \\((2N,)\\).</li> <li>All values are clipped to \\([0, 1]\\).</li> </ul> <p>For the default two-vehicle scenario, the observation has shape \\((4,)\\): <code>[v_head, v_agent, p_head, p_agent]</code>.</p>"},{"location":"problem/#13-action-space","title":"1.3 Action Space","text":"<p>The native action space is a continuous scalar acceleration:</p> \\[ a \\in [a_{\\min},\\; a_{\\max}] = [-3.0,\\; 3.0]\\;\\text{m/s}^2 \\] <p>The acceleration is integrated into velocity each step:</p> \\[ v_{t+1} = \\text{clip}\\bigl(v_t + a \\cdot \\Delta t,\\; 0,\\; v_{\\max}\\bigr) \\] <p>and applied to the vehicle via TraCI's <code>setSpeed()</code> for immediate response.</p>"},{"location":"problem/#discretization","title":"Discretization","text":"<p>Because both Q-learning and DQN operate over discrete actions, the continuous range is discretized into \\(n\\) evenly spaced bins using <code>DiscretizeActionWrapper</code>:</p> \\[ \\mathcal{A} = \\bigl\\{a_{\\min},\\;\\; a_{\\min} + \\delta,\\;\\; a_{\\min} + 2\\delta,\\;\\;\\dots,\\;\\; a_{\\max}\\bigr\\}, \\quad \\delta = \\frac{a_{\\max} - a_{\\min}}{n - 1} \\]"},{"location":"problem/#14-reward-function","title":"1.4 Reward Function","text":"<p>The active reward function is based on the DeeP-LCC [1] cost formulation, which penalizes deviations from equilibrium velocity, desired spacing, and excessive control effort.</p>"},{"location":"problem/#components","title":"Components","text":"<p>Velocity error \u2014 penalizes deviation from the target speed (\\(v^{*} = 15\\;\\text{m/s}\\)):</p> \\[ R_v = -w_v \\cdot (v_{\\text{ego}} - v^{*})^2 \\] <p>Spacing error \u2014 penalizes deviation from the OVM equilibrium spacing \\(s^{*}\\):</p> \\[ R_s = -w_s \\cdot \\bigl(\\text{clip}(d_{\\text{gap}} - s^{*},\\;-20,\\;20)\\bigr)^2 \\] <p>where \\(s^{*}\\) is computed using the Optimal Velocity Model (OVM):</p> \\[ s^{*} = \\frac{\\arccos\\!\\bigl(1 - 2\\,v^{*}/v_{\\max}\\bigr)}{\\pi}\\,(s_{\\text{go}} - s_{\\text{st}}) + s_{\\text{st}} \\] <p>with \\(s_{\\text{st}} = 5\\;\\text{m}\\) (stop spacing) and \\(s_{\\text{go}} = 35\\;\\text{m}\\) (free-flow spacing).</p> <p>Control effort \u2014 penalizes large accelerations for smooth driving:</p> \\[ R_u = -w_u \\cdot a^2 \\] <p>Safety constraint \u2014 a hard penalty when the gap falls below a minimum:</p> \\[ R_{\\text{safety}} = \\begin{cases} -100 &amp; \\text{if } d_{\\text{gap}} &lt; s_{\\min} \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\]"},{"location":"problem/#total-reward","title":"Total Reward","text":"\\[ R = \\frac{R_v + R_s + R_u + R_{\\text{safety}}}{100} \\] <p>The division by 100 scales per-step values into a range suitable for policy gradient methods.</p>"},{"location":"problem/#default-weights","title":"Default Weights","text":"Parameter Symbol Default Velocity \\(w_v\\) 0.8 Spacing \\(w_s\\) 0.7 Control \\(w_u\\) 0.1 Min gap \\(s_{\\min}\\) 5.0 m"},{"location":"problem/#references","title":"References","text":"<p>[1] Wang, J., Zheng, Y., Li, K., &amp; Xu, Q. (2023). DeeP-LCC: Data-EnablEd Predictive Leading Cruise Control in Mixed Traffic Flow. IEEE Transactions on Control Systems Technology, 31(6), 2760\u20132776. doi:10.1109/tcst.2023.3288636</p>"}]}