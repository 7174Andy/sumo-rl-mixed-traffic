{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"SUMO RL Mixed Traffic","text":"<p>Reinforcement learning for traffic flow optimization using SUMO (Simulation of Urban Mobility).</p>"},{"location":"#overview","title":"Overview","text":"<p>This project implements RL agents that control a single autonomous vehicle in a ring road scenario to maximize traffic flow by learning optimal speed control policies.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.13+</li> <li>SUMO installed with <code>SUMO_HOME</code> environment variable set</li> </ul>"},{"location":"#installation","title":"Installation","text":"<pre><code>uv sync\n</code></pre>"},{"location":"#training","title":"Training","text":"<p>Q-Learning:</p> <pre><code>uv run rl_mixed_traffic/q_train.py\n</code></pre> <p>DQN:</p> <pre><code>uv run rl_mixed_traffic/dqn_train.py\n</code></pre>"},{"location":"#evaluation","title":"Evaluation","text":"<p>Q-Learning:</p> <pre><code>uv run rl_mixed_traffic/q_eval_policy.py\n</code></pre> <p>DQN:</p> <pre><code>uv run rl_mixed_traffic/dqn_eval.py\n</code></pre>"},{"location":"algorithms/","title":"Algorithms","text":""},{"location":"algorithms/#1-tabular-q-learning","title":"1. Tabular Q-Learning","text":""},{"location":"algorithms/#11-overview","title":"1.1 Overview","text":"<p>Q-learning is a model-free, off-policy reinforcement learning algorithm that learns an action-value function \\(Q(s, a)\\) mapping state-action pairs to expected cumulative reward. It is \"tabular\" because \\(Q\\) is stored as a lookup table (implemented as a <code>defaultdict</code>) rather than approximated by a neural network.</p> <p>Tabular Q-learning is a natural first baseline for this problem because the discretized state and action spaces are small enough to enumerate, updates are simple and stable, and there are no neural-network hyperparameters to tune.</p>"},{"location":"algorithms/#12-state-discretization","title":"1.2 State Discretization","text":"<p>The continuous observation vector \\(\\mathbf{s} \\in [0, 1]^{2N}\\) must be converted to a discrete key for table lookup. The <code>StateDiscretizer</code> partitions each dimension into \\(B\\) bins using edge arrays and maps each continuous value to a bin index via <code>np.digitize</code>. The result is an integer tuple of length \\(2N\\):</p> \\[ \\mathbf{s}_{\\text{disc}} = \\bigl(b_0,\\; b_1,\\; \\dots,\\; b_{2N-1}\\bigr), \\quad b_i \\in \\{0, \\dots, B-1\\} \\] <p>Two binning modes are available:</p> Mode Dimensions Spacing Use case Uniform (default) All Equal-width bins across \\([0, 1]\\) General purpose Position heavy-tail Position dims only Quadratic spacing (\\(u^2\\)) concentrating bins near 0 Higher resolution at small headways <p>In the default training configuration, \\(B = 20\\) bins per dimension are used for both velocity and position dimensions.</p>"},{"location":"algorithms/#13-update-rule","title":"1.3 Update Rule","text":"<p>After each transition \\((s, a, r, s')\\), the Q-value is updated using the standard off-policy TD(0) rule:</p> \\[ Q(s, a) \\leftarrow (1 - \\alpha)\\,Q(s, a) + \\alpha\\,\\bigl[r + \\gamma \\max_{a'} Q(s', a')\\bigr] \\] <p>where:</p> <ul> <li>\\(\\alpha\\) is the learning rate controlling how much new information overrides the old estimate.</li> <li>\\(\\gamma\\) is the discount factor weighting future rewards.</li> <li>\\(\\max_{a'} Q(s', a')\\) is the best estimated value from the next state (set to 0 at terminal states).</li> </ul> <p>This is equivalent to the more commonly written form:</p> \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha\\,\\bigl[r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)\\bigr] \\]"},{"location":"algorithms/#14-exploration-strategy","title":"1.4 Exploration Strategy","text":"<p>The agent uses an \\(\\varepsilon\\)-greedy policy with linear decay:</p> \\[ \\varepsilon(t) = \\varepsilon_{\\text{start}} + \\frac{t}{T_{\\text{decay}}} \\cdot (\\varepsilon_{\\text{end}} - \\varepsilon_{\\text{start}}) \\] <p>where \\(t\\) is the current step count and \\(T_{\\text{decay}}\\) is the total decay horizon. At each step:</p> <ul> <li>With probability \\(\\varepsilon\\), the agent selects a random action (exploration).</li> <li>With probability \\(1 - \\varepsilon\\), the agent selects \\(\\arg\\max_a Q(s, a)\\) (exploitation), breaking ties randomly.</li> </ul> <p>Epsilon starts at 1.0 (fully random) and decays linearly to 0.05 over the full training run, encouraging broad exploration early and convergence to the learned policy later.</p>"},{"location":"algorithms/#15-hyperparameters","title":"1.5 Hyperparameters","text":"Parameter Symbol Default Learning rate \\(\\alpha\\) 0.2 Discount factor \\(\\gamma\\) 0.98 Initial epsilon \\(\\varepsilon_{\\text{start}}\\) 1.0 Final epsilon \\(\\varepsilon_{\\text{end}}\\) 0.05 Epsilon decay steps \\(T_{\\text{decay}}\\) <code>num_episodes * max_steps</code> Action bins \\(n\\) 20 State bins per dim \\(B\\) 20 Episodes \u2014 250"},{"location":"algorithms/#16-problems-and-limitations","title":"1.6 Problems and Limitations","text":"<p>For this specific problem of controlling CAV, tabular Q-learning algorithm has several limitations:</p> <ol> <li> <p>Scalability: The state-action space grows exponentially with the number of vehicles and discretization bins, leading to a large Q-table that may not fit in memory or converge within a reasonable time.</p> </li> <li> <p>Generalization: The agent cannot generalize to unseen states or actions, which may occur frequently in a continuous environment.</p> </li> <li> <p>Loss of precision from discretization: The discretization process may lose important information about the state, especially if the bins are too coarse. This can lead to suboptimal policies.</p> </li> </ol> <p>Therefore, we concluded to use a Deep Q-Network (DQN) that can handle continuous state spaces and generalize better, while still being sample efficient and stable for training.</p>"},{"location":"algorithms/#2-deep-q-network-dqn","title":"2. Deep Q-Network (DQN)","text":""},{"location":"algorithms/#21-overview","title":"2.1 Overview","text":"<p>Deep Q-Networks (DQN) replace the Q-table with a neural network \\(Q_\\theta(s, a)\\) that maps continuous state vectors directly to action values. This addresses the key limitations of tabular Q-learning identified above: the network generalizes across similar states without requiring explicit discretization, and its parameter count is fixed regardless of the state space size.</p> <p>The implementation uses Double DQN [1] with a separate target network and experience replay buffer for stable training.</p>"},{"location":"algorithms/#22-network-architecture","title":"2.2 Network Architecture","text":"<p>The Q-network is a two-layer MLP that maps the observation vector to Q-values for each discrete action:</p> \\[ \\mathbf{s} \\in \\mathbb{R}^{2N} \\xrightarrow{\\text{Linear}(2N, 128)} \\xrightarrow{\\text{ReLU}} \\xrightarrow{\\text{Linear}(128, |\\mathcal{A}|)} \\mathbf{q} \\in \\mathbb{R}^{|\\mathcal{A}|} \\] Layer Input dim Output dim Activation <code>fc1</code> \\(2N\\) 128 ReLU <code>out</code> 128 \\(\\lvert\\mathcal{A}\\rvert\\) None (linear) <p>For the default 4-vehicle, 21-action configuration: input dim = 8, output dim = 21, total parameters \\(\\approx\\) 3,900.</p>"},{"location":"algorithms/#23-double-dqn","title":"2.3 Double DQN","text":"<p>Standard DQN uses the same network to both select and evaluate the next-state action, which leads to systematic overestimation of Q-values. Double DQN decouples these two steps:</p> <ol> <li>The online network \\(Q_\\theta\\) selects the best next action:</li> </ol> \\[ a^* = \\arg\\max_{a'} Q_\\theta(s', a') \\] <ol> <li>The target network \\(Q_{\\theta^-}\\) evaluates that action:</li> </ol> \\[ y = r + \\gamma\\,(1 - d)\\,Q_{\\theta^-}(s', a^*) \\] <p>where \\(d = 1\\) if the episode terminated, \\(d = 0\\) otherwise. This reduces overestimation bias by preventing the same network from both proposing and scoring the greedy action.</p>"},{"location":"algorithms/#24-experience-replay","title":"2.4 Experience Replay","text":"<p>Transitions \\((s, a, r, s', d)\\) are stored in a fixed-capacity replay buffer implemented as a <code>deque</code> with maximum size 200,000. When the buffer is full, the oldest transitions are discarded.</p> <p>During each learning step, a uniform random batch of 128 transitions is sampled. This breaks temporal correlations between consecutive samples and allows each transition to be reused across multiple updates, improving sample efficiency.</p> <p>Learning begins only after the buffer accumulates at least 10,000 transitions (<code>start_learning_after</code>), ensuring the initial batches are sufficiently diverse.</p>"},{"location":"algorithms/#25-target-network","title":"2.5 Target Network","text":"<p>A separate target network \\(Q_{\\theta^-}\\) provides stable regression targets during training. It is updated every 3,000 steps using soft updates (Polyak averaging):</p> \\[ \\theta^- \\leftarrow (1 - \\tau)\\,\\theta^- + \\tau\\,\\theta \\] <p>with \\(\\tau = 0.01\\) by default. This gradually blends the online network's weights into the target, providing a smoother learning signal than periodic hard copies (\\(\\tau = 1.0\\)).</p> <p>When \\(\\tau = 1.0\\), the update becomes a hard copy of the online network weights.</p>"},{"location":"algorithms/#26-training-details","title":"2.6 Training Details","text":"<p>Loss function: Smooth L1 (Huber) loss between predicted and target Q-values:</p> \\[ \\mathcal{L} = \\text{SmoothL1}\\bigl(Q_\\theta(s, a),\\; y\\bigr) \\] <p>Huber loss behaves as MSE for small errors and as MAE for large errors, making it more robust to outlier transitions than pure MSE.</p> <p>Optimizer: Adam with learning rate \\(5 \\times 10^{-4}\\).</p> <p>Gradient clipping: Gradients are clipped by global norm to 10.0 via <code>clip_grad_norm_</code> to prevent destabilizing parameter updates.</p> <p>Training loop: The agent operates in a step-based loop (not episode-based). At each of the 350,000 total steps:</p> <ol> <li>Select action via \\(\\varepsilon\\)-greedy (same linear decay as Q-learning).</li> <li>Execute action in SUMO, observe \\((r, s', d)\\).</li> <li>Store the transition in the replay buffer.</li> <li>If buffer size \\(\\geq\\) 10,000 and <code>steps % train_freq == 0</code>: sample a batch and perform one gradient step.</li> <li>If <code>steps % target_update_freq == 0</code>: update the target network.</li> <li>On episode termination, reset the environment and log the return.</li> </ol>"},{"location":"algorithms/#27-hyperparameters","title":"2.7 Hyperparameters","text":"Parameter Symbol Default Learning rate \\(\\eta\\) \\(5 \\times 10^{-4}\\) Discount factor \\(\\gamma\\) 0.99 Batch size \u2014 128 Buffer size \u2014 200,000 Start learning after \u2014 10,000 steps Train frequency \u2014 Every 1 step Target update freq \u2014 Every 3,000 steps Soft update factor \\(\\tau\\) 0.01 Initial epsilon \\(\\varepsilon_{\\text{start}}\\) 1.0 Final epsilon \\(\\varepsilon_{\\text{end}}\\) 0.10 Epsilon decay steps \\(T_{\\text{decay}}\\) 500,000 Max gradient norm \u2014 10.0 Action bins \\(\\lvert\\mathcal{A}\\rvert\\) 21 Total training steps \u2014 350,000"},{"location":"algorithms/#28-results","title":"2.8 Results","text":""},{"location":"algorithms/#29-problems-and-limitations","title":"2.9 Problems and Limitations","text":"<p>Still, the action space is discretized, which may limit the optimality of the learned policy. The agent cannot output fine-grained accelerations, which may be necessary for smooth control in a continuous environment. As shown in the DQN performance plot, the agent learns a reasonable policy but displays some spikes in the velocity to catch up the head vehicle, which may not be ideal for smooth traffic flow. Therefore, we concluded to use a policy-gradient method that can directly output continuous actions, such as Proximal Policy Optimization (PPO), which is more suitable for continuous control tasks and can learn more stable policies with fewer hyperparameters to tune.</p>"},{"location":"algorithms/#3-proximal-policy-optimization-ppo","title":"3. Proximal Policy Optimization (PPO)","text":""},{"location":"algorithms/#31-overview","title":"3.1 Overview","text":"<p>PPO is an on-policy, policy-gradient algorithm that directly optimizes a parameterized policy \\(\\pi_\\theta(a \\mid s)\\) rather than learning action values. Unlike Q-learning and DQN, PPO naturally handles continuous action spaces \u2014 the policy outputs a Gaussian distribution over accelerations, eliminating the need for action discretization and enabling fine-grained, smooth control.</p> <p>The implementation uses an Actor-Critic architecture with a clipped surrogate objective and Generalized Advantage Estimation (GAE) [2].</p>"},{"location":"algorithms/#32-network-architecture","title":"3.2 Network Architecture","text":"<p>The Actor-Critic network shares feature extraction layers between the policy (actor) and the value function (critic):</p> \\[ \\mathbf{s} \\in \\mathbb{R}^{2N} \\xrightarrow{\\text{Linear}(2N, 256)} \\xrightarrow{\\text{ReLU}} \\xrightarrow{\\text{Linear}(256, 256)} \\xrightarrow{\\text{ReLU}} \\begin{cases} \\xrightarrow{\\text{Actor head}} \\mu \\in \\mathbb{R}^{d_a} \\\\ \\xrightarrow{\\text{Critic head}} V \\in \\mathbb{R} \\end{cases} \\] Layer Input dim Output dim Activation <code>shared1</code> \\(2N\\) 256 ReLU <code>shared2</code> 256 256 ReLU <code>actor_mean</code> 256 \\(d_a\\) None <code>critic_head</code> 256 1 None <p>Gaussian policy: The actor outputs a mean \\(\\mu_\\theta(s)\\) and uses a learnable, state-independent log standard deviation \\(\\log \\sigma\\) (clamped to \\([-2.0, 0.5]\\)). Actions are sampled from \\(\\mathcal{N}(\\mu, \\sigma^2)\\) and squashed through \\(\\tanh\\) to produce bounded outputs in \\([-1, 1]\\), which are then rescaled to the acceleration range \\([-3, 3]\\) m/s\u00b2.</p> <p>The log-probability is corrected for the tanh change of variables:</p> \\[ \\log \\pi(a \\mid s) = \\log \\mu(u \\mid s) - \\sum_i \\log(1 - \\tanh^2(u_i)) \\] <p>where \\(u\\) is the pre-squashed sample.</p>"},{"location":"algorithms/#33-clipped-surrogate-objective","title":"3.3 Clipped Surrogate Objective","text":"<p>PPO constrains policy updates using a clipped probability ratio to prevent destructively large steps:</p> \\[ L^{\\text{CLIP}} = \\mathbb{E}\\Bigl[\\min\\bigl(r_t \\hat{A}_t,\\;\\; \\text{clip}(r_t, 1-\\epsilon, 1+\\epsilon)\\,\\hat{A}_t\\bigr)\\Bigr] \\] <p>where:</p> <ul> <li>\\(r_t = \\frac{\\pi_\\theta(a_t \\mid s_t)}{\\pi_{\\theta_\\text{old}}(a_t \\mid s_t)}\\) is the probability ratio between new and old policies.</li> <li>\\(\\hat{A}_t\\) is the estimated advantage (see Section 3.4).</li> <li>\\(\\epsilon = 0.2\\) is the clipping threshold.</li> </ul> <p>The \\(\\min\\) ensures that when the advantage is positive, the ratio cannot exceed \\(1 + \\epsilon\\), and when negative, it cannot fall below \\(1 - \\epsilon\\). This creates a trust region that keeps the updated policy close to the data-collecting policy.</p>"},{"location":"algorithms/#34-generalized-advantage-estimation-gae","title":"3.4 Generalized Advantage Estimation (GAE)","text":"<p>Advantages are computed using GAE, which provides a tunable bias-variance tradeoff via the \\(\\lambda\\) parameter:</p> \\[ \\hat{A}_t = \\sum_{l=0}^{T-t-1} (\\gamma \\lambda)^l \\delta_{t+l} \\] <p>where the TD residual is:</p> \\[ \\delta_t = r_t + \\gamma\\,(1 - d_t)\\,V(s_{t+1}) - V(s_t) \\] <ul> <li>When \\(\\lambda = 0\\): pure one-step TD (low variance, high bias).</li> <li>When \\(\\lambda = 1\\): Monte Carlo return (high variance, low bias).</li> <li>Default \\(\\lambda = 0.95\\) balances both.</li> </ul> <p>Returns for the value function target are computed as \\(R_t = \\hat{A}_t + V(s_t)\\).</p>"},{"location":"algorithms/#35-training-loop","title":"3.5 Training Loop","text":"<p>PPO uses a rollout-based training loop that alternates between data collection and policy updates:</p> <ol> <li>Collect rollout: Run the current policy for 2,048 steps, storing \\((s, a, r, V(s), \\log\\pi(a|s), d)\\) in the rollout buffer.</li> <li>Compute advantages: Apply GAE over the collected rollout, bootstrapping from \\(V(s_T)\\) if the episode did not terminate.</li> <li>Normalize advantages: Subtract mean and divide by standard deviation for training stability.</li> <li>Minibatch updates: For \\(K = 10\\) epochs, shuffle the rollout data and iterate over minibatches of size 64:</li> <li>Recompute \\(\\log\\pi_\\theta(a|s)\\) and \\(V_\\theta(s)\\) under the current parameters.</li> <li>Compute the clipped policy loss, value loss (MSE), and entropy bonus.</li> <li>Minimize the combined loss: \\(L = L^{\\text{CLIP}} + c_v L^{\\text{value}} + c_e L^{\\text{entropy}}\\).</li> <li>Clip gradients by global norm to 0.5.</li> <li>Clear buffer and repeat from step 1.</li> </ol>"},{"location":"algorithms/#36-loss-function","title":"3.6 Loss Function","text":"<p>The total loss combines three terms:</p> \\[ L = L^{\\text{CLIP}} + c_v \\cdot \\text{MSE}\\bigl(V_\\theta(s),\\; R_t\\bigr) - c_e \\cdot H[\\pi_\\theta(\\cdot \\mid s)] \\] Term Coefficient Purpose Policy loss 1.0 Maximize clipped advantage Value loss \\(c_v = 0.5\\) Fit value function to returns Entropy bonus \\(c_e = 0.01\\) Encourage exploration, prevent collapse"},{"location":"algorithms/#37-hyperparameters","title":"3.7 Hyperparameters","text":"Parameter Symbol Default Learning rate \\(\\eta\\) \\(3 \\times 10^{-4}\\) Discount factor \\(\\gamma\\) 0.99 GAE lambda \\(\\lambda\\) 0.95 Clip epsilon \\(\\epsilon\\) 0.2 Epochs per update \\(K\\) 10 Minibatch size \u2014 64 Value coefficient \\(c_v\\) 0.5 Entropy coefficient \\(c_e\\) 0.01 Max gradient norm \u2014 0.5 Rollout steps \u2014 2,048 Total training steps \u2014 600,000 Hidden dim \u2014 256"},{"location":"algorithms/#38-results","title":"3.8 Results","text":""},{"location":"algorithms/#references","title":"References","text":"<p>[1] Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2013). Playing Atari with Deep Reinforcement Learning. arXiv [Cs.LG]. Retrieved from http://arxiv.org/abs/1312.5602</p> <p>[2] Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). Proximal Policy Optimization Algorithms. arXiv [Cs.LG]. Retrieved from http://arxiv.org/abs/1707.06347</p>"},{"location":"problem/","title":"Problem Formulation","text":""},{"location":"problem/#11-environment-ring-road-mixed-traffic","title":"1.1 Environment: Ring Road Mixed Traffic","text":"<p>This project studies a single-lane ring road where one RL-controlled autonomous vehicle (CAV) shares the road with a human-driven head vehicle. The ring topology eliminates boundary effects (on-ramps, traffic lights) and isolates the core car-following dynamics.</p> <p>The head vehicle (<code>car0</code>) periodically changes its cruising speed at random, creating disturbances that propagate through the ring. The AV (<code>car1</code>) must learn a speed-control policy that maintains safe following distances, tracks the traffic flow, and avoids abrupt acceleration changes.</p> <p>The problem is formulated as a Markov Decision Process (MDP):</p> Component Description State Normalized velocities and positions of all vehicles on the ring Action Longitudinal acceleration command applied to the CAV Reward Quadratic cost penalizing velocity error, spacing error, and control effort Transition Determined by SUMO's car-following models and the applied acceleration <p>The simulation runs at a time step of \\(\\Delta t = 0.1\\,\\text{s}\\), and each episode lasts up to 500 s (5,000 steps).</p>"},{"location":"problem/#12-state-space","title":"1.2 State Space","text":"<p>The observation is a fixed-length vector of normalized velocities and positions for all \\(N\\) vehicles:</p> \\[ \\mathbf{s} = \\bigl[\\underbrace{v_0/v_{\\max},\\;\\dots,\\;v_{N-1}/v_{\\max}}_{\\text{velocities}},\\;\\underbrace{p_0/L,\\;\\dots,\\;p_{N-1}/L}_{\\text{positions}}\\bigr] \\] <p>where:</p> <ul> <li>\\(v_{\\max} = 30\\;\\text{m/s}\\) is the maximum speed.</li> <li>\\(L\\) is the ring circumference (computed from the SUMO network at startup).</li> <li>Vehicles are sorted by ID so each index always maps to the same vehicle.</li> <li>If fewer than \\(N\\) vehicles are present, the vector is zero-padded to maintain a fixed shape of \\((2N,)\\).</li> <li>All values are clipped to \\([0, 1]\\).</li> </ul> <p>For the default two-vehicle scenario, the observation has shape \\((4,)\\): <code>[v_head, v_agent, p_head, p_agent]</code>.</p>"},{"location":"problem/#13-action-space","title":"1.3 Action Space","text":"<p>The native action space is a continuous scalar acceleration:</p> \\[ a \\in [a_{\\min},\\; a_{\\max}] = [-3.0,\\; 3.0]\\;\\text{m/s}^2 \\] <p>The acceleration is integrated into velocity each step:</p> \\[ v_{t+1} = \\text{clip}\\bigl(v_t + a \\cdot \\Delta t,\\; 0,\\; v_{\\max}\\bigr) \\] <p>and applied to the vehicle via TraCI's <code>setSpeed()</code> for immediate response.</p>"},{"location":"problem/#discretization","title":"Discretization","text":"<p>Because both Q-learning and DQN operate over discrete actions, the continuous range is discretized into \\(n\\) evenly spaced bins using <code>DiscretizeActionWrapper</code>:</p> \\[ \\mathcal{A} = \\bigl\\{a_{\\min},\\;\\; a_{\\min} + \\delta,\\;\\; a_{\\min} + 2\\delta,\\;\\;\\dots,\\;\\; a_{\\max}\\bigr\\}, \\quad \\delta = \\frac{a_{\\max} - a_{\\min}}{n - 1} \\]"},{"location":"problem/#14-reward-function","title":"1.4 Reward Function","text":"<p>The active reward function is based on the DeeP-LCC [1] cost formulation, which penalizes deviations from equilibrium velocity, desired spacing, and excessive control effort.</p>"},{"location":"problem/#components","title":"Components","text":"<p>Velocity error \u2014 penalizes deviation from the target speed (\\(v^{*} = 15\\;\\text{m/s}\\)):</p> \\[ R_v = -w_v \\cdot (v_{\\text{ego}} - v^{*})^2 \\] <p>Spacing error \u2014 penalizes deviation from the OVM equilibrium spacing \\(s^{*}\\):</p> \\[ R_s = -w_s \\cdot \\bigl(\\text{clip}(d_{\\text{gap}} - s^{*},\\;-20,\\;20)\\bigr)^2 \\] <p>where \\(s^{*}\\) is computed using the Optimal Velocity Model (OVM):</p> \\[ s^{*} = \\frac{\\arccos\\!\\bigl(1 - 2\\,v^{*}/v_{\\max}\\bigr)}{\\pi}\\,(s_{\\text{go}} - s_{\\text{st}}) + s_{\\text{st}} \\] <p>with \\(s_{\\text{st}} = 5\\;\\text{m}\\) (stop spacing) and \\(s_{\\text{go}} = 35\\;\\text{m}\\) (free-flow spacing).</p> <p>Control effort \u2014 penalizes large accelerations for smooth driving:</p> \\[ R_u = -w_u \\cdot a^2 \\] <p>Safety constraint \u2014 a hard penalty when the gap falls below a minimum:</p> \\[ R_{\\text{safety}} = \\begin{cases} -100 &amp; \\text{if } d_{\\text{gap}} &lt; s_{\\min} \\\\ 0 &amp; \\text{otherwise} \\end{cases} \\]"},{"location":"problem/#total-reward","title":"Total Reward","text":"\\[ R = \\frac{R_v + R_s + R_u + R_{\\text{safety}}}{100} \\] <p>The division by 100 scales per-step values into a range suitable for policy gradient methods.</p>"},{"location":"problem/#default-weights","title":"Default Weights","text":"Parameter Symbol Default Velocity \\(w_v\\) 0.8 Spacing \\(w_s\\) 0.7 Control \\(w_u\\) 0.1 Min gap \\(s_{\\min}\\) 5.0 m"},{"location":"problem/#references","title":"References","text":"<p>[1] Wang, J., Zheng, Y., Li, K., &amp; Xu, Q. (2023). DeeP-LCC: Data-EnablEd Predictive Leading Cruise Control in Mixed Traffic Flow. IEEE Transactions on Control Systems Technology, 31(6), 2760\u20132776. doi:10.1109/tcst.2023.3288636</p>"}]}