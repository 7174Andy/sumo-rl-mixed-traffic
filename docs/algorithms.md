# Algorithms

## 1. Tabular Q-Learning

### 1.1 Overview

<!-- High-level description of tabular Q-learning and why it applies here. -->

### 1.2 State Discretization

<!-- Explain uniform binning, number of bins per dimension, and the position heavy-tail mode. -->

### 1.3 Update Rule

<!-- Present the Q-learning update equation and explain each term. -->

### 1.4 Exploration Strategy

<!-- Describe epsilon-greedy exploration with linear decay schedule. -->

### 1.5 Hyperparameters

<!-- Table of key hyperparameters: learning rate, discount factor, epsilon schedule, number of episodes. -->

---

## 2. Deep Q-Network (DQN)

### 2.1 Overview

<!-- High-level description of DQN and motivation for using it over tabular methods. -->

### 2.2 Network Architecture

<!-- Describe the 2-layer MLP: input size, hidden sizes, activation functions, output size. -->

### 2.3 Double DQN

<!-- Explain the Double DQN modification and how it reduces overestimation bias. -->

### 2.4 Experience Replay

<!-- Describe the replay buffer, sampling strategy, and buffer size. -->

### 2.5 Target Network

<!-- Explain hard vs. soft target network updates and update frequency. -->

### 2.6 Training Details

<!-- Loss function (Huber/SmoothL1), optimizer, gradient clipping, and training loop structure. -->

### 2.7 Hyperparameters

<!-- Table of key hyperparameters: learning rate, batch size, buffer size, epsilon schedule, target update frequency. -->

## References

<!-- List relevant papers and resources. -->
